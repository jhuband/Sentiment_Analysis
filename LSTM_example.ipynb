{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  LSTM Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Keras Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/jmh5ad/Sentiment_Analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDir = \"/scratch/\" + os.getenv('USER') + \"/Sentiment_Analysis\"\n",
    "os.chdir(myDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
      "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
      "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
      "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
      "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('Sentiment.csv')\n",
    "\n",
    "# Keeping only the necessary columns\n",
    "data = data[['text','sentiment']]\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pre-Process the Data:  Simplify\n",
    "\n",
    "We'll need to do a few tasks to simplify the texts.\n",
    "1. Let's remove the items classified as \"Neutral\" (i.e., keep anything that is not \"Neutral\");\n",
    "2. Let's remove any characters that are not alphanumeric (i.e., replace anything not alphanumeric or a space with nothing); \n",
    "3. Let's remove the \"RT\" at the beginning of the messages, and\n",
    "4. Let's convert everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "1   scottwalker didnt catch the full gopdebate la...  Positive\n",
      "3   robgeorge that carly fiorina is trending  hou...  Positive\n",
      "4   danscavino gopdebate w realdonaldtrump delive...  Positive\n",
      "5   gregabbott_tx tedcruz on my first day i will ...  Positive\n",
      "6   warriorwoman91 i liked her and was happy when...  Negative\n",
      "\n",
      "Num Positive Tweets: 4472\n",
      "Num Negative Tweets: 16986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('^RT','',x)))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "## Just to know what we are left with, let's check the # of positive or negative sentiments\n",
    "print(\"\\nNum Positive Tweets:\", data[ data['sentiment'] == 'Positive'].size)\n",
    "print(\"Num Negative Tweets:\", data[ data['sentiment'] == 'Negative'].size)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process the Data:  Create Sequences\n",
    "\n",
    "To process the data with tensorflow, we will need to convert it to a numerical representation.  This is done by creating a list of words and representing each tweet with the index of the words in that list.  \n",
    "\n",
    "Each tweet will be a _*sequence*_ of indices.  But, for mathematical processing, we will want the vectors to be the same size.  So, we will \"pad\" the vectors with zeros to ensure the proper length.\n",
    "\n",
    "Similary, we will want to convert the classifications (e.g., positive or negative) to a sequence.  This sequence is called _one hot vector_.  It is a vector of zeros and a single value of 1.  The 1 will be placed in the position corresponding to one of the classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " danscavino gopdebate w realdonaldtrump delivered the highest ratings in the history of presidential debates trump2016 httptco\n",
      "[1248, 2, 300, 23, 1928, 1, 1615, 213, 12, 1, 695, 6, 183, 204, 367, 680]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0 1248    2\n",
      "  300   23 1928    1 1615  213   12    1  695    6  183  204  367  680]\n",
      "Positive\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_words = 2000\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "\n",
    "# Let's take a peek at what we have for X and Y\n",
    "print(data['text'].values[2])\n",
    "print(X[2])\n",
    "X = pad_sequences(X)\n",
    "print(X[2])\n",
    "\n",
    "\n",
    "print(data['sentiment'].values[2])\n",
    "print(Y[2])\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split data into training and testing sets\n",
    "\n",
    "The next step is to split the data into a _*training*_ set and a _*testing*_ set.  The training set will be used to solve for the mathematical model that will be used to determine if a text is positive or negative.  The testing set is used to see how accuately the model works on data that were not used for creating the model.\n",
    "\n",
    "Depending on the size of your data, you may be able to use large subset for testing.  In this example, we set `test_size` to 0.33.  So, about 33% or 1/3 of the data will be set aside to be used for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7188, 28) (7188, 2)\n",
      "(3541, 28) (3541, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define the Model\n",
    "\n",
    "Defining the model is an art.\n",
    "\n",
    "Unless we get into very sophisticated model, we will want to start with a _Sequential_ model.  This gives us the framework for adding layer to our model.\n",
    "\n",
    "##### Embedding Layer\n",
    "The first layer in an LSTM module should be an _Embedding_ layer.  The _Embedding_ will convert the input words to vectors of values.  Each word will be represented by a vector of size given by _embed_dim_ (in this case, 128). The _Embedding_ function also needs to know the maximum number of words that we are considering as part of our corpus (max_words) and number of columns of the input (X.shape[1]).\n",
    "\n",
    "##### SpatialDropout1D Layer\n",
    "The Spatial Dropout layer will randomly choose rows from the model to remove (i.e., dropout).  When placed in an iterative loop, this step will ensure that one or two features are not dominating the entire model.  \n",
    "\n",
    "The value passed into the _SpatialDropout1D_ function represents the percentage of rows to drop for each iteration.\n",
    "\n",
    "##### LSTM Layer\n",
    "The _LSTM_ layer performs the model fitting based on the training data.In this example the function takes as input the size of the output (i.e., lstm_out), the dropout rate of the linear transformation (0.2), and the dropout rate of the recurrent state (0.2).\n",
    "\n",
    "##### Dense Layer\n",
    "The _Dense_ layer takes the outputs from the previous layer and applies a basic neural network operation to transform the values.  In this example, the activation function will determine which nodes of the neural network will be turned on or off.  Because this is the final step of our model, we want to ensure that the output of the neural network will match the total number of classifications (i.e., 2 -- positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embed_dim, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model will simply create the locations in memory for the computations that will fit the model to the data.  It does not do any of the actual computations.  That step will come after we configure the learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Configure the Learning Process\n",
    "\n",
    "One more step that is needed in to specify how the algorithm will \"learn\".  In other words, what functions will be used to measure loss, perform updates to the values.  You can learn more about different functons that can be selected at https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 28, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 28, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fit the Model to the Data\n",
    "\n",
    "We are ready to run the algorithm to fit the model to the training data.  To do this, we will give the _fit_ function the X and Y values of the training data.  We also need to tell the function how many iterations or _epochs_ we would like for the model to repeat.  Normally, the fit will improve with each epoch. Finally, we want to specify the _batch_size_.  This parameter takes a little more explanation.\n",
    "\n",
    "If we have hundreds of thousands of rows in our data, the computations to fit the model to the data would be enormous, requiring lots of computer memory.  Insead of trying to use all of the data for the fitting, the algorithm will split the data into batches and use each batch to compute results.  The _batch_size_ will be the number of rows that we want to run through the model at one time.  The parameters for the model are tweaked slightly when the results of batch run are assessed.  All of the batches will be run through the model within a single epoch.\n",
    "\n",
    "In other words, the algorithm goes through many, many iterations, making adjustments to the parameters with each iteration.  Because we want to watch where it is in the fitting process, we set the _verbose_ parameter to 1.  This parameter will print some intermediate results to the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "225/225 [==============================] - 40s 179ms/step - loss: 0.1184 - accuracy: 0.9509\n",
      "Epoch 2/7\n",
      "225/225 [==============================] - 38s 171ms/step - loss: 0.1209 - accuracy: 0.9498\n",
      "Epoch 3/7\n",
      "225/225 [==============================] - 41s 181ms/step - loss: 0.1086 - accuracy: 0.9560\n",
      "Epoch 4/7\n",
      "225/225 [==============================] - 40s 179ms/step - loss: 0.1022 - accuracy: 0.9606\n",
      "Epoch 5/7\n",
      "225/225 [==============================] - 40s 178ms/step - loss: 0.0993 - accuracy: 0.9587\n",
      "Epoch 6/7\n",
      "225/225 [==============================] - 41s 181ms/step - loss: 0.0932 - accuracy: 0.9601\n",
      "Epoch 7/7\n",
      "225/225 [==============================] - 40s 179ms/step - loss: 0.0844 - accuracy: 0.9656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5650399e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 7 \n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Apply Model to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 - 3s - loss: 0.7500 - accuracy: 0.8252\n",
      "score: 0.75\n",
      "acc: 0.83\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the model to a \"new\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
    "\n",
    "# Pre-process tweet\n",
    "twt = re.sub('[^a-zA-z0-9\\s]','',twt[0].lower())\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "twt = pad_sequences(twt, maxlen=28, value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 - 1s\n",
      "**The tweet is negative.**\n"
     ]
    }
   ],
   "source": [
    "# Run through the model\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "\n",
    "#State result\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"**The tweet is negative.**\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"**The tweet is positive,**\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.1/Keras Py3.7",
   "language": "python",
   "name": "tensorflow210_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
